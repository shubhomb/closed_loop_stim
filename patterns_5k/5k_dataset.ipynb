{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33871d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3396476, 2)\n",
      "47  unique neurons recorded\n",
      "328503.754  seconds of recording\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>neuron_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>196</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>244</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>246</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>340</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  neuron_id\n",
       "0        122        206\n",
       "1        196          5\n",
       "2        244        200\n",
       "3        246        300\n",
       "4        340        200"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pickle\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datadir = \"data/\"\n",
    "\n",
    "# 5000 patterns. Each pattern consists of indices characterized by which chnanels are modulated oen of 4 delay modes. 44 stimulating channels are used (though some pattern indices have no stimulation on any channels). \n",
    "# Each stim is 3 uA amplitude, biphasic, 167-66-167 us biphasic pulses with 66 us interphase interval. \n",
    "# approx 30kHz sampling rate but imprecise. Each index step lasts 0.06s, and 10 pattern steps are present for each pattern.\n",
    "\n",
    "# Each stimulation is delivered in 600 ms period, with 1400 inter-stimulation interval. Thus, 2 s per stimulation * 5000 stimulations = 10000 seconds total experiment.\n",
    "\n",
    "pattern_registrations = pickle.load(open(os.path.join(datadir, \"pattern_registrations.pkl\"), \"rb\"))\n",
    "\n",
    "# spkVecs contains spike times for all neurons. 47 are recorded.\n",
    "spikes_df = pd.DataFrame(np.load(os.path.join(datadir, \"spkVecs.npy\"))) # size = (3396476,))\n",
    "spikes_df.columns = ['timestamp', 'neuron_id', 'segment_index']\n",
    "spikes_df.drop(columns=['segment_index'], inplace=True)\n",
    "spikes_df['timestamp'] = spikes_df['timestamp'].astype(int)\n",
    "spikes_df['neuron_id'] = spikes_df['neuron_id'].astype(int) # noncontinuous neuron ids from with numbers corresponding to shank location\n",
    "spikes_df.sort_values(by=['timestamp', 'neuron_id'], inplace=True)\n",
    "print (spikes_df.shape)\n",
    "print (spikes_df['neuron_id'].nunique(), \" unique neurons recorded\")\n",
    "print (spikes_df['timestamp'].max()/1000, \" seconds of recording\")\n",
    "spikes_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "923cb331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_pattern_json(data):\n",
    "    # 1. Flatten the Pattern -> Steps level\n",
    "    # We use record_path to reach the 'steps' and meta to keep parent info\n",
    "    df = pd.json_normalize(\n",
    "        data, \n",
    "        record_path=['steps'], \n",
    "        meta=[\n",
    "            'pattern_name', # given name of the pattern\n",
    "            'pattern_lambda', # given lambda parameter that generated the pattern \n",
    "            'pattern_flag_start_timestamp', # given starting timpestamp of pattern\n",
    "            'pattern_timing_index' # given order of pattern from 1-5,000 (first to last)\n",
    "        ],\n",
    "        record_prefix='step_'\n",
    "    )\n",
    "    # 2. Flatten the 'step_channel_delays' list into individual rows\n",
    "    # This creates a row for every delay entry. Steps with [] will become NaN.\n",
    "    df = df.explode('step_channel_delays').reset_index(drop=True)\n",
    "\n",
    "    # 3. Convert the dictionaries in 'step_channel_delays' into separate columns\n",
    "    delays_df = pd.json_normalize(df['step_channel_delays'])\n",
    "    \n",
    "    # 4. Store pattern length\n",
    "    df['pattern_idx_length'] = df.groupby('pattern_timing_index')['step_index'].transform('max') + 1\n",
    "\n",
    "    # 6. Combine and cleanup\n",
    "    final_df = pd.concat([df.drop(columns=['step_channel_delays']), delays_df], axis=1)\n",
    "    \n",
    "    # 7. Convert timestamps to integers and subtract original timestamp from step timestamps\n",
    "    final_df['step_start_timestamp'] = final_df['step_start_timestamp'].astype(int)\n",
    "    final_df['pattern_flag_start_timestamp'] = final_df['pattern_flag_start_timestamp'].astype(int)\n",
    "\n",
    "    return final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ee0b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/38/_qz39whn0hngmtsmjg9byfbr0000gn/T/ipykernel_24765/624276988.py:14: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  patterns['pattern_end_timestamp'].iloc[-1] = last_spike_time\n",
      "/var/folders/38/_qz39whn0hngmtsmjg9byfbr0000gn/T/ipykernel_24765/624276988.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patterns['pattern_end_timestamp'].iloc[-1] = last_spike_time\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step_index</th>\n",
       "      <th>step_start_timestamp</th>\n",
       "      <th>pattern_name</th>\n",
       "      <th>pattern_lambda</th>\n",
       "      <th>pattern_flag_start_timestamp</th>\n",
       "      <th>pattern_timing_index</th>\n",
       "      <th>pattern_idx_length</th>\n",
       "      <th>channel</th>\n",
       "      <th>delay_mode</th>\n",
       "      <th>pattern_end_timestamp</th>\n",
       "      <th>pattern_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3891</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5691</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7491</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>7491</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>9771</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>9771</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>11718</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>37.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>11718</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>13628</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>13628</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>13628</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>13628</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>15428</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>17461</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>47.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>17461</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>19261</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>21211</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>21211</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9</td>\n",
       "      <td>21211</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>21211</td>\n",
       "      <td>2961</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62201</td>\n",
       "      <td>62201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>65296</td>\n",
       "      <td>3029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62202</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123676</td>\n",
       "      <td>61474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>65296</td>\n",
       "      <td>3029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62202</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123676</td>\n",
       "      <td>61474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>65296</td>\n",
       "      <td>3029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62202</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>123676</td>\n",
       "      <td>61474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>65296</td>\n",
       "      <td>3029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62202</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123676</td>\n",
       "      <td>61474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>67119</td>\n",
       "      <td>3029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62202</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123676</td>\n",
       "      <td>61474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>67119</td>\n",
       "      <td>3029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62202</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123676</td>\n",
       "      <td>61474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>67119</td>\n",
       "      <td>3029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62202</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>123676</td>\n",
       "      <td>61474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>67119</td>\n",
       "      <td>3029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62202</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123676</td>\n",
       "      <td>61474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>69005</td>\n",
       "      <td>3029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62202</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>123676</td>\n",
       "      <td>61474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>69005</td>\n",
       "      <td>3029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62202</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123676</td>\n",
       "      <td>61474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    step_index  step_start_timestamp pattern_name pattern_lambda  \\\n",
       "0            0                  3891         2961            0.8   \n",
       "1            1                  5691         2961            0.8   \n",
       "2            2                  7491         2961            0.8   \n",
       "3            2                  7491         2961            0.8   \n",
       "4            3                  9771         2961            0.8   \n",
       "5            3                  9771         2961            0.8   \n",
       "6            4                 11718         2961            0.8   \n",
       "7            4                 11718         2961            0.8   \n",
       "8            5                 13628         2961            0.8   \n",
       "9            5                 13628         2961            0.8   \n",
       "10           5                 13628         2961            0.8   \n",
       "11           5                 13628         2961            0.8   \n",
       "12           6                 15428         2961            0.8   \n",
       "13           7                 17461         2961            0.8   \n",
       "14           7                 17461         2961            0.8   \n",
       "15           8                 19261         2961            0.8   \n",
       "16           9                 21211         2961            0.8   \n",
       "17           9                 21211         2961            0.8   \n",
       "18           9                 21211         2961            0.8   \n",
       "19           9                 21211         2961            0.8   \n",
       "20           0                 65296         3029            1.0   \n",
       "21           0                 65296         3029            1.0   \n",
       "22           0                 65296         3029            1.0   \n",
       "23           0                 65296         3029            1.0   \n",
       "24           1                 67119         3029            1.0   \n",
       "25           1                 67119         3029            1.0   \n",
       "26           1                 67119         3029            1.0   \n",
       "27           1                 67119         3029            1.0   \n",
       "28           2                 69005         3029            1.0   \n",
       "29           2                 69005         3029            1.0   \n",
       "\n",
       "    pattern_flag_start_timestamp pattern_timing_index  pattern_idx_length  \\\n",
       "0                              0                    0                  10   \n",
       "1                              0                    0                  10   \n",
       "2                              0                    0                  10   \n",
       "3                              0                    0                  10   \n",
       "4                              0                    0                  10   \n",
       "5                              0                    0                  10   \n",
       "6                              0                    0                  10   \n",
       "7                              0                    0                  10   \n",
       "8                              0                    0                  10   \n",
       "9                              0                    0                  10   \n",
       "10                             0                    0                  10   \n",
       "11                             0                    0                  10   \n",
       "12                             0                    0                  10   \n",
       "13                             0                    0                  10   \n",
       "14                             0                    0                  10   \n",
       "15                             0                    0                  10   \n",
       "16                             0                    0                  10   \n",
       "17                             0                    0                  10   \n",
       "18                             0                    0                  10   \n",
       "19                             0                    0                  10   \n",
       "20                         62202                    1                  10   \n",
       "21                         62202                    1                  10   \n",
       "22                         62202                    1                  10   \n",
       "23                         62202                    1                  10   \n",
       "24                         62202                    1                  10   \n",
       "25                         62202                    1                  10   \n",
       "26                         62202                    1                  10   \n",
       "27                         62202                    1                  10   \n",
       "28                         62202                    1                  10   \n",
       "29                         62202                    1                  10   \n",
       "\n",
       "    channel  delay_mode  pattern_end_timestamp  pattern_duration  \n",
       "0       NaN         NaN                  62201             62201  \n",
       "1       NaN         NaN                  62201             62201  \n",
       "2      17.0         1.0                  62201             62201  \n",
       "3      79.0         0.0                  62201             62201  \n",
       "4      23.0         1.0                  62201             62201  \n",
       "5      36.0         0.0                  62201             62201  \n",
       "6      37.0        -1.0                  62201             62201  \n",
       "7      57.0         0.0                  62201             62201  \n",
       "8      24.0         0.0                  62201             62201  \n",
       "9      63.0         0.0                  62201             62201  \n",
       "10     28.0         1.0                  62201             62201  \n",
       "11     61.0         0.0                  62201             62201  \n",
       "12      NaN         NaN                  62201             62201  \n",
       "13     47.0        -1.0                  62201             62201  \n",
       "14     54.0         0.0                  62201             62201  \n",
       "15      NaN         NaN                  62201             62201  \n",
       "16     23.0         0.0                  62201             62201  \n",
       "17     47.0         0.0                  62201             62201  \n",
       "18     16.0         0.0                  62201             62201  \n",
       "19     54.0         0.0                  62201             62201  \n",
       "20      5.0         0.0                 123676             61474  \n",
       "21     62.0         0.0                 123676             61474  \n",
       "22     28.0         1.0                 123676             61474  \n",
       "23     37.0         0.0                 123676             61474  \n",
       "24     50.0         0.0                 123676             61474  \n",
       "25     73.0         0.0                 123676             61474  \n",
       "26      8.0        -1.0                 123676             61474  \n",
       "27     51.0         0.0                 123676             61474  \n",
       "28      1.0        -1.0                 123676             61474  \n",
       "29     60.0         0.0                 123676             61474  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_df = read_pattern_json(pattern_registrations)\n",
    "\n",
    "# define pattern end as pattern's start time - 1\n",
    "# build unique patterns and set end = next pattern's start - 1\n",
    "patterns = (pattern_df.groupby(['pattern_timing_index', 'pattern_name'])['pattern_flag_start_timestamp']\n",
    "            .first().reset_index())\n",
    "patterns = patterns.sort_values('pattern_timing_index').reset_index(drop=True)\n",
    "\n",
    "# last spike time for final end\n",
    "last_spike_time = spikes_df['timestamp'].max()\n",
    "\n",
    "# end = next start - 1\n",
    "patterns['pattern_end_timestamp'] = patterns['pattern_flag_start_timestamp'].shift(-1) - 1\n",
    "patterns['pattern_end_timestamp'].iloc[-1] = last_spike_time\n",
    "\n",
    "# ensure integer timestamps\n",
    "patterns['pattern_end_timestamp'] = patterns['pattern_end_timestamp'].astype(int)\n",
    "patterns['pattern_flag_start_timestamp'] = patterns['pattern_flag_start_timestamp'].astype(int)\n",
    "\n",
    "# merge end times back into the full step-level dataframe\n",
    "pattern_df = pattern_df.merge(\n",
    "    patterns[['pattern_timing_index', 'pattern_end_timestamp']],\n",
    "    on='pattern_timing_index',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# how many times each pattern appears in separate timestamps\n",
    "pattern_counts = pattern_df[['pattern_name', 'pattern_timing_index']].drop_duplicates().groupby('pattern_name').size()\n",
    "min_pattern_timestamp = pattern_df['pattern_flag_start_timestamp'].min()\n",
    "# Center times to 0 for easier intepretation\n",
    "pattern_df['pattern_flag_start_timestamp'] -= min_pattern_timestamp\n",
    "pattern_df['pattern_end_timestamp'] -= min_pattern_timestamp\n",
    "pattern_df['pattern_duration'] = pattern_df['pattern_end_timestamp'] - pattern_df['pattern_flag_start_timestamp']\n",
    "pattern_df['step_start_timestamp'] -= min_pattern_timestamp\n",
    "spikes_df['timestamp'] -= min_pattern_timestamp\n",
    "pattern_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_to_index = {ch: idx for idx, ch in enumerate(sorted(pattern_df['channel'].dropna().unique()))}\n",
    "print (\"Channel indices to channel mapping:\")\n",
    "for ch, idx in channel_to_index.items():\n",
    "    print (f\"Channel {ch}: index {idx}\")\n",
    "\n",
    "spiking_neurons = spikes_df['neuron_id'].unique()\n",
    "spiking_neurons.sort()\n",
    "print (f\"Total spiking neurons recorded: {len(spiking_neurons)}\")\n",
    "print (\"Spiking neuron names: \", spiking_neurons)\n",
    "spiking_neuron_to_index = {neuron: idx for idx, neuron in enumerate(spiking_neurons)}\n",
    "print (\"Spiking neuron to index mapping:\")\n",
    "for neuron, idx in spiking_neuron_to_index.items():\n",
    "    print (f\"Spiking neuron {neuron}: index {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dd60dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1sAAAIhCAYAAAC48qAWAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAT6VJREFUeJzt3XlclPX+///ngIBAgKICkmtGpuGSWi6VS6aWoucc62NmYZqmncq9NNvEvh5N65ilabappdk5lZ52FMslA5dUTD1km0suiAuCIgIy798f/pjjCCgDXA0jj/vtxu3W9b7e13W9ZuaNzZP3Ne+xGWOMAAAAAADlysvdBQAAAADAlYiwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAMVYuHChbDab46dq1aqKiIhQly5dNG3aNKWlpRU6Ji4uTjabzaXrnDlzRnFxcVqzZo1LxxV1rQYNGigmJsal81zOBx98oFmzZhW5z2azKS4urlyvV96++eYbtWnTRoGBgbLZbPrPf/5TZL+9e/c6vd5eXl6qUaOGevbsqaSkJJev+9VXXxX73EydOrXYOtxt0KBBTs9DYGCgGjRooD59+mjBggXKyclxa32HDh1SXFyckpOTC+0rze8fAFiJsAUAl7FgwQIlJSUpISFBr7/+ulq2bKnp06erSZMmWrVqlVPfoUOHuvzG/MyZM5o8ebLLYas01yqNS4WtpKQkDR061PIaSssYo379+snHx0efffaZkpKS1KlTp0seM2LECCUlJem7777TtGnTtH37dnXp0kXbtm1z6dpfffWVJk+eXOS+ihy2JMnf319JSUlKSkrSF198oRdeeEGBgYF6+OGH1bp1ax04cMBttR06dEiTJ08uMmz9Wb8TAFBSVdxdAABUdNHR0WrTpo1j++6779aYMWN06623qm/fvvrll18UHh4uSapTp47q1KljaT1nzpxRQEDAn3Kty2nXrp1br385hw4d0okTJ/S3v/1NXbt2LdEx9erVczyuW265Rddee626du2quXPn6q233rKy3DLJz8/XuXPn5OfnV+ZzeXl5FXptBw4cqMGDBysmJkb33HOPNmzYUObrSFJ2draqVq1aLjNSFeF3AgAuxMwWAJRCvXr19M9//lOnTp3S/PnzHe1F3cb07bffqnPnzqpRo4b8/f1Vr1493X333Tpz5oz27t2rWrVqSZImT57suHVr0KBBTufbunWr7rnnHlWvXl2NGjUq9loFli9frubNm6tq1aq65ppr9NprrzntL7hFcu/evU7ta9askc1mc8yyde7cWV9++aX27dvndGtZgaJuI9y5c6f+8pe/qHr16qpatapatmypRYsWFXmdpUuX6plnnlFkZKSCg4N1xx13aPfu3cU/8RdYv369unbtqqCgIAUEBKhDhw768ssvHfvj4uIcb7wnTJggm82mBg0alOjcFyoIHfv27ZMk/etf/1L37t1Vu3Zt+fv7q0mTJnrqqaeUlZXlOGbQoEF6/fXXJcnpeSu4VTErK0uLFi1ytHfu3NlxbGpqqoYPH646derI19dXDRs21OTJk3Xu3DlHn4LzzJgxQ1OmTFHDhg3l5+en1atXO8bFrl27dN999ykkJETh4eF66KGHlJGR4fLjv1D37t318MMPa+PGjVq3bp2jvbjbSRs0aOAYy9L/xt3KlSv10EMPqVatWgoICFBOTo5+/fVXDR48WFFRUQoICNDVV1+t3r17a8eOHY7j16xZo5tuukmSNHjwYMfzV3Dton4n7Ha7ZsyYoeuvv15+fn4KCwvTwIEDC83Ode7cWdHR0dq8ebNuu+02BQQE6JprrtGLL74ou93udL4pU6aocePG8vf3V7Vq1dS8eXO9+uqrpX1aAVzBmNkCgFLq2bOnvL29nd50Xmzv3r3q1auXbrvtNr377ruqVq2aDh48qPj4eOXm5qp27dqKj4/XnXfeqSFDhjhuySsIYAX69u2r/v3765FHHnF6U1+U5ORkjR49WnFxcYqIiNCSJUs0atQo5ebm6oknnnDpMc6dO1fDhg3Tb7/9puXLl1+2/+7du9WhQweFhYXptddeU40aNbR48WINGjRIR44c0fjx4536P/3007rlllv09ttvKzMzUxMmTFDv3r2VkpIib2/vYq+zdu1adevWTc2bN9c777wjPz8/zZ07V71799bSpUt17733aujQoWrRooX69u2rESNGaMCAAaWa9fn1118l/e81+eWXX9SzZ0+NHj1agYGB+umnnzR9+nRt2rRJ3377rSTpueeeU1ZWlj7++GOn29pq166tpKQk3X777erSpYuee+45SVJwcLCk80Hr5ptvlpeXl55//nk1atRISUlJmjJlivbu3asFCxY41fbaa6/puuuu08svv6zg4GBFRUU5Zpzuvvtu3XvvvRoyZIh27NihiRMnSpLeffddl5+DC/Xp00dz587VunXr1LFjx1Kd46GHHlKvXr30/vvvKysrSz4+Pjp06JBq1KihF198UbVq1dKJEye0aNEitW3bVtu2bVPjxo3VqlUrLViwQIMHD9azzz6rXr16SdIlZ7P+/ve/680339Tjjz+umJgY7d27V88995zWrFmjrVu3qmbNmo6+qampuv/++zVu3DhNmjRJy5cv18SJExUZGamBAwdKkmbMmKG4uDg9++yz6tixo/Ly8vTTTz/p5MmTpXouAFzhDACgSAsWLDCSzObNm4vtEx4ebpo0aeLYnjRpkrnwn9aPP/7YSDLJycnFnuPo0aNGkpk0aVKhfQXne/7554vdd6H69esbm81W6HrdunUzwcHBJisry+mx7dmzx6nf6tWrjSSzevVqR1uvXr1M/fr1i6z94rr79+9v/Pz8zP79+5363XXXXSYgIMCcPHnS6To9e/Z06vfvf//bSDJJSUlFXq9Au3btTFhYmDl16pSj7dy5cyY6OtrUqVPH2O12Y4wxe/bsMZLMSy+9dMnzXdh3+vTpJi8vz5w9e9Zs2bLF3HTTTUaS+fLLLwsdY7fbTV5enlm7dq2RZLZv3+7Y99hjjxV6fQoEBgaaBx98sFD78OHDzVVXXWX27dvn1P7yyy8bSWbXrl1OtTZq1Mjk5uY69S0YFzNmzHBqf/TRR03VqlUdz01xHnzwQRMYGFjs/pSUFCPJ/P3vf3e0FTd+69ev7/Q4C8bdwIEDL1mDMedfz9zcXBMVFWXGjBnjaN+8ebORZBYsWFDomIt/JwpqffTRR536bdy40UgyTz/9tKOtU6dORpLZuHGjU9+mTZuaHj16OLZjYmJMy5YtL1s/ABhjDLcRAkAZGGMuub9ly5by9fXVsGHDtGjRIv3++++lus7dd99d4r433HCDWrRo4dQ2YMAAZWZmauvWraW6fkl9++236tq1q+rWrevUPmjQIJ05c6bQ4gV9+vRx2m7evLmk/92yV5SsrCxt3LhR99xzj6666ipHu7e3t2JjY3XgwIES34pYlAkTJsjHx0dVq1ZV69attX//fs2fP189e/aUJP3+++8aMGCAIiIi5O3tLR8fH8eiGykpKaW+riR98cUX6tKliyIjI3Xu3DnHz1133SXp/Izehfr06SMfH58iz1XUc3v27NkiV9F0xeXGfEkUNZ7PnTunqVOnqmnTpvL19VWVKlXk6+urX375pdTP6+rVqyXJ6VZGSbr55pvVpEkTffPNN07tERERuvnmm53amjdv7jQeb775Zm3fvl2PPvqoVqxYoczMzFLVBqBy4DZCACilrKwsHT9+XM2aNSu2T6NGjbRq1SrNmDFDjz32mLKysnTNNddo5MiRGjVqVImvVbt27RL3jYiIKLbt+PHjJT5PaRw/frzIWiMjI4u8fo0aNZy2C27zy87OLvYa6enpMsa4dB1XjBo1Sg888IC8vLxUrVo1NWzY0PE5oNOnT+u2225T1apVNWXKFF133XUKCAjQH3/8ob59+16y7pI4cuSIPv/882ID1LFjx5y2LzUuSvPclkRB8Ch4rkujqLrHjh2r119/XRMmTFCnTp1UvXp1eXl5aejQoaWuuWAcFDdWLg71Fz9n0vnn7cLrT5w4UYGBgVq8eLHeeOMNeXt7q2PHjpo+fbrTQjoAIBG2AKDUvvzyS+Xn5zstblCU2267Tbfddpvy8/P1ww8/aPbs2Ro9erTCw8PVv3//El3LlZXaUlNTi20reDNZtWpVSSr0nUkXv5l3VY0aNXT48OFC7YcOHZIkp8/HlFbBm3CrrlOnTp1i3zR/++23OnTokNasWeO0hHx5fV6nZs2aat68uf7xj38Uuf/igOOO75T67LPPJMlp3Pv5+RX5/VvFhd6i6l68eLEGDhyoqVOnOrUfO3ZM1apVK1WtBeP98OHDhT7XdejQoVKNkypVqmjs2LEaO3asTp48qVWrVunpp59Wjx499McffyggIKBUtQK4MnEbIQCUwv79+/XEE08oJCREw4cPL9Ex3t7eatu2rWOVuoJb+sprxqHArl27tH37dqe2Dz74QEFBQWrVqpUkOVbl+/HHH536FbyRvtDFf9m/lK5duzoCyYXee+89BQQElMtS8YGBgWrbtq2WLVvmVJfdbtfixYtVp04dXXfddWW+TlEKQsLFC21cuCJlgUu9rsU9pzExMdq5c6caNWqkNm3aFPopy2xSeUhISNDbb7+tDh066NZbb3W0N2jQoNBY+vbbb3X69OkSn9tmsxV6Xr/88ksdPHjQqc2V35fbb79d0vkgd6HNmzcrJSWlxF8HUJxq1arpnnvu0WOPPaYTJ04UWt0TAJjZAoDL2Llzp+OzM2lpafruu++0YMECeXt7a/ny5YVWDrzQG2+8oW+//Va9evVSvXr1dPbsWcdqcHfccYckKSgoSPXr19enn36qrl27KjQ0VDVr1izVMuXS+dmPPn36KC4uTrVr19bixYuVkJCg6dOnO/7qftNNN6lx48Z64okndO7cOVWvXl3Lly/X+vXrC52vWbNmWrZsmebNm6fWrVvLy8ur2JmfSZMmOT539Pzzzys0NFRLlizRl19+qRkzZigkJKRUj+li06ZNU7du3dSlSxc98cQT8vX11dy5c7Vz504tXbrUshmfDh06qHr16nrkkUc0adIk+fj4aMmSJYXCrSTH7aXTp0/XXXfdJW9vbzVv3ly+vr5q1qyZ1qxZo88//1y1a9dWUFCQGjdurBdeeEEJCQnq0KGDRo4cqcaNG+vs2bPau3evvvrqK73xxht/yvdI2e12x6qGOTk52r9/v77++mv9+9//VpMmTfTvf//bqX9sbKyee+45Pf/88+rUqZP++9//as6cOS693jExMVq4cKGuv/56NW/eXFu2bNFLL71U6PE2atRI/v7+WrJkiZo0aaKrrrpKkZGRRQbRxo0ba9iwYZo9e7a8vLx01113OVYjrFu3rsaMGePyc9O7d2/Hd+/VqlVL+/bt06xZs1S/fn1FRUW5fD4AVzg3L9ABABVWwcppBT++vr4mLCzMdOrUyUydOtWkpaUVOubi1dCSkpLM3/72N1O/fn3j5+dnatSoYTp16mQ+++wzp+NWrVplbrzxRuPn52ckOVZwKzjf0aNHL3stY86v/tarVy/z8ccfmxtuuMH4+vqaBg0amJkzZxY6/ueffzbdu3c3wcHBplatWmbEiBHmyy+/LLQa4YkTJ8w999xjqlWrZmw2m9M1VcQqdDt27DC9e/c2ISEhxtfX17Ro0aLQynEFqxF+9NFHTu0Fq+wVtdLcxb777jtz++23m8DAQOPv72/atWtnPv/88yLP58pqhJfrm5iYaNq3b28CAgJMrVq1zNChQ83WrVsL1Z2Tk2OGDh1qatWq5XjeClZ/TE5ONrfccosJCAgwkkynTp0cxx09etSMHDnSNGzY0Pj4+JjQ0FDTunVr88wzz5jTp09fttbixkxxK1Be7MEHH3Qa9/7+/qZevXqmd+/e5t133zU5OTmFjsnJyTHjx483devWNf7+/qZTp04mOTm52NUIi1rhMz093QwZMsSEhYWZgIAAc+utt5rvvvvOdOrUyen5McaYpUuXmuuvv974+Pg4jcGifify8/PN9OnTzXXXXWd8fHxMzZo1zQMPPGD++OMPp36dOnUyN9xwQ5HPx4Wrcf7zn/80HTp0MDVr1jS+vr6mXr16ZsiQIWbv3r2XfF4BVE42Y8phWSEAAAAAgBM+swUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABfhS4xKy2+06dOiQgoKCLPuyTAAAAAAVnzFGp06dUmRkpLy8ip+/ImyV0KFDh1S3bl13lwEAAACggvjjjz9Up06dYvcTtkooKChI0vknNDg42M3VVCx5eXlauXKlunfvLh8fH3eXAw/D+EFZMH5QFowflAXjp3LLzMxU3bp1HRmhOIStEiq4dTA4OJiwdZG8vDwFBAQoODiYf2zgMsYPyoLxg7Jg/KAsGD+QdNmPF7FABgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWqOLuAlA6+/fv17Fjx9xdhiTJbrdLkrZv3y4vL/J7RVWzZk3Vq1fP3WUAAABUGoQtD7R//35d36SJss+ccXcpkiR/f38tXbpUHTt2VHZ2trvLQTH8AwL0U0oKgQsAAOBPQtjyQMeOHVP2mTPqN2WewhpGubscectIytKwtz9TvmzuLgdFSNvzi/797N917NgxwhYAAMCfhLDlwcIaRunqJi3cXYa87OekAxsV2Thadi+GFAAAACCxQAYAAAAAWIKwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWKDChK1p06bJZrNp9OjRjjZjjOLi4hQZGSl/f3917txZu3btcjouJydHI0aMUM2aNRUYGKg+ffrowIEDTn3S09MVGxurkJAQhYSEKDY2VidPnvwTHhUAAACAyqpChK3NmzfrzTffVPPmzZ3aZ8yYoZkzZ2rOnDnavHmzIiIi1K1bN506dcrRZ/To0Vq+fLk+/PBDrV+/XqdPn1ZMTIzy8/MdfQYMGKDk5GTFx8crPj5eycnJio2N/dMeHwAAAIDKx+1h6/Tp07r//vv11ltvqXr16o52Y4xmzZqlZ555Rn379lV0dLQWLVqkM2fO6IMPPpAkZWRk6J133tE///lP3XHHHbrxxhu1ePFi7dixQ6tWrZIkpaSkKD4+Xm+//bbat2+v9u3b66233tIXX3yh3bt3u+UxAwAAALjyVXF3AY899ph69eqlO+64Q1OmTHG079mzR6mpqerevbujzc/PT506dVJiYqKGDx+uLVu2KC8vz6lPZGSkoqOjlZiYqB49eigpKUkhISFq27ato0+7du0UEhKixMRENW7cuMi6cnJylJOT49jOzMyUJOXl5SkvL6/cHn9p2O12+fv7y1tGXvZzbq1FkqOGilALiuYtI39/f9ntdreP34sV1FPR6oJnYPygLBg/KAvGT+VW0tfdrWHrww8/1NatW7V58+ZC+1JTUyVJ4eHhTu3h4eHat2+fo4+vr6/TjFhBn4LjU1NTFRYWVuj8YWFhjj5FmTZtmiZPnlyofeXKlQoICLjMI7Pe0qVLJWVJBza6uxSHqENb3F0CitE4UOqydKkOHjyogwcPurucIiUkJLi7BHgwxg/KgvGDsmD8VE5nzpwpUT+3ha0//vhDo0aN0sqVK1W1atVi+9lsNqdtY0yhtotd3Keo/pc7z8SJEzV27FjHdmZmpurWravu3bsrODj4kte32vbt29WxY0cNe/szRTaOdmst0vkZrahDW/RLZGvZvdw+WYoiHNq9U28O7aN169apRYsW7i7HSV5enhISEtStWzf5+Pi4uxx4GMYPyoLxg7Jg/FRuBXe9XY7b3hlv2bJFaWlpat26taMtPz9f69at05w5cxyfp0pNTVXt2rUdfdLS0hyzXREREcrNzVV6errT7FZaWpo6dOjg6HPkyJFC1z969GihWbML+fn5yc/Pr1C7j4+P23+hvLy8lJ2drXzZKlS4sXtVqVD14H/yZVN2dra8vLzcPn6LUxF+t+C5GD8oC8YPyoLxUzmV9DV32wIZXbt21Y4dO5ScnOz4adOmje6//34lJyfrmmuuUUREhNPUbG5urtauXesIUq1bt5aPj49Tn8OHD2vnzp2OPu3bt1dGRoY2bdrk6LNx40ZlZGQ4+gAAAABAeXPbNERQUJCio51vgQsMDFSNGjUc7aNHj9bUqVMVFRWlqKgoTZ06VQEBARowYIAkKSQkREOGDNG4ceNUo0YNhYaG6oknnlCzZs10xx13SJKaNGmiO++8Uw8//LDmz58vSRo2bJhiYmKKXRwDAAAAAMqqQt/zNX78eGVnZ+vRRx9Venq62rZtq5UrVyooKMjR55VXXlGVKlXUr18/ZWdnq2vXrlq4cKG8vb0dfZYsWaKRI0c6Vi3s06eP5syZ86c/HgAAAACVR4UKW2vWrHHattlsiouLU1xcXLHHVK1aVbNnz9bs2bOL7RMaGqrFixeXU5UAAAAAcHlu/1JjAAAAALgSEbYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAs4HLY2rp1q3bs2OHY/vTTT/XXv/5VTz/9tHJzc8u1OAAAAADwVC6HreHDh+vnn3+WJP3+++/q37+/AgIC9NFHH2n8+PHlXiAAAAAAeCKXw9bPP/+sli1bSpI++ugjdezYUR988IEWLlyoTz75pLzrAwAAAACP5HLYMsbIbrdLklatWqWePXtKkurWratjx46Vb3UAAAAA4KFcDltt2rTRlClT9P7772vt2rXq1auXJGnPnj0KDw8v9wIBAAAAwBO5HLZmzZqlrVu36vHHH9czzzyja6+9VpL08ccfq0OHDi6da968eWrevLmCg4MVHBys9u3b6+uvv3bsN8YoLi5OkZGR8vf3V+fOnbVr1y6nc+Tk5GjEiBGqWbOmAgMD1adPHx04cMCpT3p6umJjYxUSEqKQkBDFxsbq5MmTrj50AAAAACgxl8NW8+bNtWPHDmVkZGjSpEmO9pdeekmLFi1y6Vx16tTRiy++qB9++EE//PCDbr/9dv3lL39xBKoZM2Zo5syZmjNnjjZv3qyIiAh169ZNp06dcpxj9OjRWr58uT788EOtX79ep0+fVkxMjPLz8x19BgwYoOTkZMXHxys+Pl7JycmKjY119aEDAAAAQIlVKe2Bubm5SktLc3x+q0C9evVKfI7evXs7bf/jH//QvHnztGHDBjVt2lSzZs3SM888o759+0qSFi1apPDwcH3wwQcaPny4MjIy9M477+j999/XHXfcIUlavHix6tatq1WrVqlHjx5KSUlRfHy8NmzYoLZt20qS3nrrLbVv3167d+9W48aNS/sUAAAAAECxXA5bP//8s4YMGaLExESndmOMbDab04ySK/Lz8/XRRx8pKytL7du31549e5Samqru3bs7+vj5+alTp05KTEzU8OHDtWXLFuXl5Tn1iYyMVHR0tBITE9WjRw8lJSUpJCTEEbQkqV27dgoJCVFiYmKxYSsnJ0c5OTmO7czMTElSXl6e8vLySvUYy4vdbpe/v7+8ZeRlP+fWWiQ5aqgItaBo3jLy9/eX3W53+/i9WEE9Fa0ueAbGD8qC8YOyYPxUbiV93V0OW4MHD1aVKlX0xRdfqHbt2rLZbC4Xd6EdO3aoffv2Onv2rK666iotX75cTZs2dYS5ixfdCA8P1759+yRJqamp8vX1VfXq1Qv1SU1NdfQJCwsrdN2wsDBHn6JMmzZNkydPLtS+cuVKBQQEuPYgLbB06VJJWdKBje4uxSHq0BZ3l4BiNA6UuixdqoMHD+rgwYPuLqdICQkJ7i4BHozxg7Jg/KAsGD+V05kzZ0rUz+WwlZycrC1btuj66693uaiiNG7cWMnJyTp58qQ++eQTPfjgg1q7dq1j/8VhrmAG7VIu7lNU/8udZ+LEiRo7dqxjOzMzU3Xr1lX37t0VHBx82cdlpe3bt6tjx44a9vZnimwc7dZapPMzWlGHtuiXyNaye5X6zlRY6NDunXpzaB+tW7dOLVq0cHc5TvLy8pSQkKBu3brJx8fH3eXAwzB+UBaMH5QF46dyK7jr7XJcfmfctGnTcv0+LV9fX8eKhm3atNHmzZv16quvasKECZLOz0zVrl3b0T8tLc0x2xUREaHc3Fylp6c7zW6lpaU5VkaMiIjQkSNHCl336NGjl1yq3s/PT35+foXafXx83P4L5eXlpezsbOXLVqHCjd2rSoWqB/+TL5uys7Pl5eXl9vFbnIrwuwXPxfhBWTB+UBaMn8qppK+5y6sRTp8+XePHj9eaNWt0/PhxZWZmOv2UlTFGOTk5atiwoSIiIpymZnNzc7V27VpHkGrdurV8fHyc+hw+fFg7d+509Gnfvr0yMjK0adMmR5+NGzcqIyPD5aXqAQAAAKCkXJ6GKFj1r2vXrk7tpVkg4+mnn9Zdd92lunXr6tSpU/rwww+1Zs0axcfHy2azafTo0Zo6daqioqIUFRWlqVOnKiAgQAMGDJAkhYSEaMiQIRo3bpxq1Kih0NBQPfHEE2rWrJmjziZNmujOO+/Uww8/rPnz50uShg0bppiYGFYiBAAAAGAZl8PW6tWry+3iR44cUWxsrA4fPqyQkBA1b95c8fHx6tatmyRp/Pjxys7O1qOPPqr09HS1bdtWK1euVFBQkOMcr7zyiqpUqaJ+/fopOztbXbt21cKFC+Xt7e3os2TJEo0cOdKxamGfPn00Z86ccnscAAAAAHAxl8JWXl6e4uLiNH/+fF133XVlvvg777xzyf02m01xcXGKi4srtk/VqlU1e/ZszZ49u9g+oaGhWrx4cWnLBAAAAACXufSZLR8fH+3cubPMy70DAAAAwJXO5QUyBg4ceNkZKQAAAACo7Fz+zFZubq7efvttJSQkqE2bNgoMDHTaP3PmzHIrDgAAAAA8lctha+fOnWrVqpUk6eeff3bax+2FAAAAAHCeW1cjBAAAAIArlcuf2Srw66+/asWKFcrOzpZ0/nu2AAAAAADnuRy2jh8/rq5du+q6665Tz549dfjwYUnS0KFDNW7cuHIvEAAAAAA8kctha8yYMfLx8dH+/fsVEBDgaL/33nsVHx9frsUBAAAAgKdy+TNbK1eu1IoVK1SnTh2n9qioKO3bt6/cCgMAAAAAT+byzFZWVpbTjFaBY8eOyc/Pr1yKAgAAAABP53LY6tixo9577z3Hts1mk91u10svvaQuXbqUa3EAAAAA4Klcvo3wpZdeUufOnfXDDz8oNzdX48eP165du3TixAl9//33VtQIAAAAAB7H5Zmtpk2b6scff9TNN9+sbt26KSsrS3379tW2bdvUqFEjK2oEAAAAAI/j8szW/v37VbduXU2ePLnIffXq1SuXwgAAAADAk7k8s9WwYUMdPXq0UPvx48fVsGHDcikKAAAAADydy2HLGCObzVao/fTp06patWq5FAUAAAAAnq7EtxGOHTtW0vnVB5977jmn5d/z8/O1ceNGtWzZstwLBAAAAABPVOKwtW3bNknnZ7Z27NghX19fxz5fX1+1aNFCTzzxRPlXCAAAAAAeqMRha/Xq1ZKkwYMH69VXX1VwcLBlRQEAAACAp3P5M1s2m63Iz2xlZWXpoYceKpeiAAAAAMDTuRy2Fi1apOzs7ELt2dnZeu+998qlKAAAAADwdCW+jTAzM1PGGBljdOrUKaeVB/Pz8/XVV18pLCzMkiIBAAAAwNOUOGxVq1bNcQvhddddV2i/zWYr8ouOAQAAAKAycmmBDGOMbr/9dn3yyScKDQ117PP19VX9+vUVGRlpSZEAAAAA4GlKHLY6deokSdqzZ4/q1q0rLy+XP+4FAAAAAJVGicNWgfr160uSzpw5o/379ys3N9dpf/PmzcunMgAAAADwYC6HraNHj2rw4MH6+uuvi9yfn59f5qIAAAAAwNO5fC/g6NGjlZ6erg0bNsjf31/x8fFatGiRoqKi9Nlnn1lRIwAAAAB4HJdntr799lt9+umnuummm+Tl5aX69eurW7duCg4O1rRp09SrVy8r6gQAAAAAj+LyzFZWVpbj+7RCQ0N19OhRSVKzZs20devW8q0OAAAAADyUy2GrcePG2r17tySpZcuWmj9/vg4ePKg33nhDtWvXLvcCAQAAAMATuXwb4ejRo3Xo0CFJ0qRJk9SjRw8tWbJEvr6+WrhwYXnXBwAAAAAeyeWwdf/99zv++8Ybb9TevXv1008/qV69eqpZs2a5FgcAAAAAnqrEtxGeOXNGjz32mK6++mqFhYVpwIABOnbsmAICAtSqVSuCFgAAAABcoMRha9KkSVq4cKF69eql/v37KyEhQX//+9+trA0AAAAAPFaJbyNctmyZ3nnnHfXv31+S9MADD+iWW25Rfn6+vL29LSsQAAAAADxRiWe2/vjjD912222O7ZtvvllVqlRxLJYBAAAAAPifEoet/Px8+fr6OrVVqVJF586dK/eiAAAAAMDTlfg2QmOMBg0aJD8/P0fb2bNn9cgjjygwMNDRtmzZsvKtEAAAAAA8UInD1oMPPlio7YEHHijXYgAAAADgSlHisLVgwQIr6wAAAACAK0qJP7MFAAAAACg5whYAAAAAWICwBQAAAAAWIGwBAAAAgAVKFLZatWql9PR0SdILL7ygM2fOWFoUAAAAAHi6EoWtlJQUZWVlSZImT56s06dPW1oUAAAAAHi6Ei393rJlSw0ePFi33nqrjDF6+eWXddVVVxXZ9/nnny/XAgEAAADAE5UobC1cuFCTJk3SF198IZvNpq+//lpVqhQ+1GazEbYAAAAAQCUMW40bN9aHH34oSfLy8tI333yjsLAwSwsDAAAAAE9WorB1IbvdbkUdAAAAAHBFcTlsSdJvv/2mWbNmKSUlRTabTU2aNNGoUaPUqFGj8q4PAAAAADySy9+ztWLFCjVt2lSbNm1S8+bNFR0drY0bN+qGG25QQkKCFTUCAAAAgMdxeWbrqaee0pgxY/Tiiy8Wap8wYYK6detWbsUBAAAAgKdyeWYrJSVFQ4YMKdT+0EMP6b///W+5FAUAAAAAns7lsFWrVi0lJycXak9OTmaFQgAAAAD4/7l8G+HDDz+sYcOG6ffff1eHDh1ks9m0fv16TZ8+XePGjbOiRgAAAADwOC6Hreeee05BQUH65z//qYkTJ0qSIiMjFRcXp5EjR5Z7gQAAAADgiVwOWzabTWPGjNGYMWN06tQpSVJQUFC5FwYAAAAAnqxU37NVgJAFAAAAAEVzeYEMAAAAAMDlEbYAAAAAwAKELQAAAACwgEthKy8vT126dNHPP/9sVT0AAAAAcEVwKWz5+Pho586dstlsVtUDAAAAAFcEl28jHDhwoN555x0ragEAAACAK4bLS7/n5ubq7bffVkJCgtq0aaPAwECn/TNnziy34gAAAADAU7kctnbu3KlWrVpJUqHPbnF7IQAAAACc53LYWr16tRV1AAAAAMAVpdRLv//6669asWKFsrOzJUnGmHIrCgAAAAA8ncth6/jx4+ratauuu+469ezZU4cPH5YkDR06VOPGjSv3AgEAAADAE7kctsaMGSMfHx/t379fAQEBjvZ7771X8fHx5VocAAAAAHgqlz+ztXLlSq1YsUJ16tRxao+KitK+ffvKrTAAAAAA8GQuz2xlZWU5zWgVOHbsmPz8/MqlKAAAAADwdC6HrY4dO+q9995zbNtsNtntdr300kvq0qVLuRYHAAAAAJ7K5dsIX3rpJXXu3Fk//PCDcnNzNX78eO3atUsnTpzQ999/b0WNAAAAAOBxXJ7Zatq0qX788UfdfPPN6tatm7KystS3b19t27ZNjRo1sqJGAAAAAPA4Ls9sSVJERIQmT55c3rUAAAAAwBWjVGErPT1d77zzjlJSUmSz2dSkSRMNHjxYoaGh5V0fAAAAAHgkl28jXLt2rRo2bKjXXntN6enpOnHihF577TU1bNhQa9eutaJGAAAAAPA4Ls9sPfbYY+rXr5/mzZsnb29vSVJ+fr4effRRPfbYY9q5c2e5FwkAAAAAnsblma3ffvtN48aNcwQtSfL29tbYsWP122+/lWtxAAAAAOCpXA5brVq1UkpKSqH2lJQUtWzZsjxqAgAAAACPV6LbCH/88UfHf48cOVKjRo3Sr7/+qnbt2kmSNmzYoNdff10vvviiNVUCAAAAgIcpUdhq2bKlbDabjDGOtvHjxxfqN2DAAN17773lVx0AAAAAeKgS3Ua4Z88e/f7779qzZ88lf37//XeXLj5t2jTddNNNCgoKUlhYmP76179q9+7dTn2MMYqLi1NkZKT8/f3VuXNn7dq1y6lPTk6ORowYoZo1ayowMFB9+vTRgQMHnPqkp6crNjZWISEhCgkJUWxsrE6ePOlSvQAAAABQUiUKW/Xr1y/xjyvWrl2rxx57TBs2bFBCQoLOnTun7t27Kysry9FnxowZmjlzpubMmaPNmzcrIiJC3bp106lTpxx9Ro8ereXLl+vDDz/U+vXrdfr0acXExCg/P9/RZ8CAAUpOTlZ8fLzi4+OVnJys2NhYl+oFAAAAgJIq1ZcaHzx4UN9//73S0tJkt9ud9o0cObLE54mPj3faXrBggcLCwrRlyxZ17NhRxhjNmjVLzzzzjPr27StJWrRokcLDw/XBBx9o+PDhysjI0DvvvKP3339fd9xxhyRp8eLFqlu3rlatWqUePXooJSVF8fHx2rBhg9q2bStJeuutt9S+fXvt3r1bjRs3Ls3TAAAAAADFcjlsLViwQI888oh8fX1Vo0YN2Ww2xz6bzeZS2LpYRkaGJCk0NFTS+dsXU1NT1b17d0cfPz8/derUSYmJiRo+fLi2bNmivLw8pz6RkZGKjo5WYmKievTooaSkJIWEhDiCliS1a9dOISEhSkxMLDJs5eTkKCcnx7GdmZkpScrLy1NeXl6pH2N5sNvt8vf3l7eMvOzn3FqLJEcNFaEWFM1bRv7+/rLb7W4fvxcrqKei1QXPwPhBWTB+UBaMn8qtpK+7y2Hr+eef1/PPP6+JEyfKy8vlleOLZYzR2LFjdeuttyo6OlqSlJqaKkkKDw936hseHq59+/Y5+vj6+qp69eqF+hQcn5qaqrCwsELXDAsLc/S52LRp0zR58uRC7StXrlRAQICLj678LV26VFKWdGCju0txiDq0xd0loBiNA6UuS5fq4MGDOnjwoLvLKVJCQoK7S4AHY/ygLBg/KAvGT+V05syZEvVzOWydOXNG/fv3L9egJUmPP/64fvzxR61fv77Qvgtnz6Tzwezitotd3Keo/pc6z8SJEzV27FjHdmZmpurWravu3bsrODj4kte22vbt29WxY0cNe/szRTaOdmst0vkZrahDW/RLZGvZvUp1Zyosdmj3Tr05tI/WrVunFi1auLscJ3l5eUpISFC3bt3k4+Pj7nLgYRg/KAvGD8qC8VO5Fdz1djkuvzMeMmSIPvroIz311FMuF1WcESNG6LPPPtO6detUp04dR3tERISk8zNTtWvXdrSnpaU5ZrsiIiKUm5ur9PR0p9mttLQ0dejQwdHnyJEjha579OjRQrNmBfz8/OTn51eo3cfHx+2/UF5eXsrOzla+bBUq3Ni9qlSoevA/+bIpOztbXl5ebh+/xakIv1vwXIwflAXjB2XB+KmcSvqau/zOeNq0aYqJiVF8fLyaNWtW6EIzZ84s8bmMMRoxYoSWL1+uNWvWqGHDhk77GzZsqIiICCUkJOjGG2+UJOXm5mrt2rWaPn26JKl169by8fFRQkKC+vXrJ0k6fPiwdu7cqRkzZkiS2rdvr4yMDG3atEk333yzJGnjxo3KyMhwBDIAAAAAKE8uh62pU6dqxYoVjkUlLner3qU89thj+uCDD/Tpp58qKCjI8fmpkJAQ+fv7y2azafTo0Zo6daqioqIUFRWlqVOnKiAgQAMGDHD0HTJkiMaNG6caNWooNDRUTzzxhJo1a+ZYnbBJkya688479fDDD2v+/PmSpGHDhikmJoaVCAEAAABYwuWwNXPmTL377rsaNGhQmS8+b948SVLnzp2d2hcsWOA4//jx45Wdna1HH31U6enpatu2rVauXKmgoCBH/1deeUVVqlRRv379lJ2dra5du2rhwoXy9vZ29FmyZIlGjhzpWLWwT58+mjNnTpkfAwAAAAAUxeWw5efnp1tuuaVcLm6MuWwfm82muLg4xcXFFdunatWqmj17tmbPnl1sn9DQUC1evLg0ZQIAAACAy1xeUnDUqFGXDDUAAAAAgFLMbG3atEnffvutvvjiC91www2FFshYtmxZuRUHAAAAAJ7K5bBVrVo19e3b14paAAAAAOCK4XLYWrBggRV1AAAAAMAVxeXPbAEAAAAALs/lma2GDRte8vu0fv/99zIVBAAAAABXApfD1ujRo5228/LytG3bNsXHx+vJJ58sr7oAAAAAwKO5HLZGjRpVZPvrr7+uH374ocwFAQAAAMCVoNw+s3XXXXfpk08+Ka/TAQAAAIBHK7ew9fHHHys0NLS8TgcAAAAAHs3l2whvvPFGpwUyjDFKTU3V0aNHNXfu3HItDgAAAAA8lcth669//avTtpeXl2rVqqXOnTvr+uuvL6+6AAAAAMCjuRy2Jk2aZEUdAAAAAHBF4UuNAQAAAMACJZ7Z8vLyuuSXGUuSzWbTuXPnylwUAAAAAHi6Eoet5cuXF7svMTFRs2fPljGmXIoCAAAAAE9X4rD1l7/8pVDbTz/9pIkTJ+rzzz/X/fffr//3//5fuRYHAAAAAJ6qVJ/ZOnTokB5++GE1b95c586dU3JyshYtWqR69eqVd30AAAAA4JFcClsZGRmaMGGCrr32Wu3atUvffPONPv/8c0VHR1tVHwAAAAB4pBLfRjhjxgxNnz5dERERWrp0aZG3FQIAAAAAzitx2Hrqqafk7++va6+9VosWLdKiRYuK7Lds2bJyKw4AAAAAPFWJw9bAgQMvu/Q7AAAAAOC8EoethQsXWlgGAAAAAFxZSrUaIQAAAADg0ghbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABgAcIWAAAAAFiAsAUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABt4atdevWqXfv3oqMjJTNZtN//vMfp/3GGMXFxSkyMlL+/v7q3Lmzdu3a5dQnJydHI0aMUM2aNRUYGKg+ffrowIEDTn3S09MVGxurkJAQhYSEKDY2VidPnrT40QEAAACozNwatrKystSiRQvNmTOnyP0zZszQzJkzNWfOHG3evFkRERHq1q2bTp065egzevRoLV++XB9++KHWr1+v06dPKyYmRvn5+Y4+AwYMUHJysuLj4xUfH6/k5GTFxsZa/vgAAAAAVF5V3Hnxu+66S3fddVeR+4wxmjVrlp555hn17dtXkrRo0SKFh4frgw8+0PDhw5WRkaF33nlH77//vu644w5J0uLFi1W3bl2tWrVKPXr0UEpKiuLj47Vhwwa1bdtWkvTWW2+pffv22r17txo3bvznPFgAAAAAlYpbw9al7NmzR6mpqerevbujzc/PT506dVJiYqKGDx+uLVu2KC8vz6lPZGSkoqOjlZiYqB49eigpKUkhISGOoCVJ7dq1U0hIiBITE4sNWzk5OcrJyXFsZ2ZmSpLy8vKUl5dX3g/XJXa7Xf7+/vKWkZf9nFtrkeSooSLUgqJ5y8jf3192u93t4/diBfVUtLrgGRg/KAvGD8qC8VO5lfR1r7BhKzU1VZIUHh7u1B4eHq59+/Y5+vj6+qp69eqF+hQcn5qaqrCwsELnDwsLc/QpyrRp0zR58uRC7StXrlRAQIBrD8YCS5culZQlHdjo7lIcog5tcXcJKEbjQKnL0qU6ePCgDh486O5yipSQkODuEuDBGD8oC8YPyoLxUzmdOXOmRP0qbNgqYLPZnLaNMYXaLnZxn6L6X+48EydO1NixYx3bmZmZqlu3rrp3767g4OCSlm+J7du3q2PHjhr29meKbBzt1lqk8zNaUYe26JfI1rJ7VfghVSkd2r1Tbw7to3Xr1qlFixbuLsdJXl6eEhIS1K1bN/n4+Li7HHgYxg/KgvGDsmD8VG4Fd71dToV9ZxwRESHp/MxU7dq1He1paWmO2a6IiAjl5uYqPT3daXYrLS1NHTp0cPQ5cuRIofMfPXq00KzZhfz8/OTn51eo3cfHx+2/UF5eXsrOzla+bBUq3Ni9qlSoevA/+bIpOztbXl5ebh+/xakIv1vwXIwflAXjB2XB+KmcSvqaV9jv2WrYsKEiIiKcpmZzc3O1du1aR5Bq3bq1fHx8nPocPnxYO3fudPRp3769MjIytGnTJkefjRs3KiMjw9EHAAAAAMqbW6chTp8+rV9//dWxvWfPHiUnJys0NFT16tXT6NGjNXXqVEVFRSkqKkpTp05VQECABgwYIEkKCQnRkCFDNG7cONWoUUOhoaF64okn1KxZM8fqhE2aNNGdd96phx9+WPPnz5ckDRs2TDExMaxECAAAAMAybg1bP/zwg7p06eLYLviM1IMPPqiFCxdq/Pjxys7O1qOPPqr09HS1bdtWK1euVFBQkOOYV155RVWqVFG/fv2UnZ2trl27auHChfL29nb0WbJkiUaOHOlYtbBPnz7FfrcXAAAAAJQHt4atzp07yxhT7H6bzaa4uDjFxcUV26dq1aqaPXu2Zs+eXWyf0NBQLV68uCylAgAAAIBLKuxntgAAAADAkxG2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsABhCwAAAAAsQNgCAAAAAAsQtgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAAAMAChC0AAAAAsEAVdxcAAKiY9u/fr2PHjrm7jEuy2+2SpO3bt8vLi78fulvNmjVVr149d5cBABUGYQuoRFJSUtxdQiG8Wa6YDh8+rHv+7/90Njvb3aVckr+/v5YuXaqOHTsqu4LXWhn4BwTop5QUAhcA/P8IW0AlcOrYEdm8vPTAAw+4u5RCeLNcsfWbMk9hDaPcXUaxvGUkZWnY258pXzZ3l1Oppe35Rf9+9u86duwYYQsl5gkz6MXhj4Xu4Wkz6IQtoBLIPpUpY7dXyDfOvFmumHZ//40S5k5TWMMoXd2khbvLKZaX/Zx0YKMiG0fL7sX/0gBPsn//fl3fpImyz5xxdymlwh8L3cPTZtD5PxNQiVTEN868Wa6Y0vb84u4SAFzhjh07puwzZyrkHwJLgj8W/vk8cQaddzYAAABwm4r4h8CS4I+FKAluMAUAAAAACxC2AAAAAMACzHkCAIByUxG/YqI4rCbnXp40VoDSImwBAIAyq8hfMVEcVpMDYLVKFbbmzp2rl156SYcPH9YNN9ygWbNm6bbbbnN3WQAAeLyK/BUTxWE1Ofcq+IoJ4EpWacLWv/71L40ePVpz587VLbfcovnz5+uuu+7Sf//7X49ZOhIAgIrOk1aWYzU59+IrJlAZVJoblGfOnKkhQ4Zo6NChatKkiWbNmqW6detq3rx57i4NAAAAwBWoUvwZJzc3V1u2bNFTTz3l1N69e3clJiYWeUxOTo5ycnIc2xkZGZKkEydOKC8vz7piSyAzM1NVq1bVkd07dO7MabfWIp2/DaNuYLb2b9vAbRgVVPofv1eoMXMhxk/FVJHHzIUYPxWHp4yZCzF+3MsTx8yFGD9/vuN/7FHVqlWVmZmp48ePu7WWU6dOSZKMMZfsZzOX63EFOHTokK6++mp9//336tChg6N96tSpWrRokXbv3l3omLi4OE2ePPnPLBMAAACAB/njjz9Up06dYvdXipmtAjab818djDGF2gpMnDhRY8eOdWzb7XadOHFCNWrUKPaYyiozM1N169bVH3/8oeDgYHeXAw/D+EFZMH5QFowflAXjp3IzxujUqVOKjIy8ZL9KEbZq1qwpb29vpaamOrWnpaUpPDy8yGP8/Pzk5+fn1FatWjWrSrwiBAcH848NSo3xg7Jg/KAsGD8oC8ZP5RUSEnLZPpVigQxfX1+1bt1aCQkJTu0JCQlOtxUCAAAAQHmpFDNbkjR27FjFxsaqTZs2at++vd58803t379fjzzyiLtLAwAAAHAFqjRh695779Xx48f1wgsv6PDhw4qOjtZXX32l+vXru7s0j+fn56dJkyYVuu0SKAnGD8qC8YOyYPygLBg/KIlKsRohAAAAAPzZKsVntgAAAADgz0bYAgAAAAALELYAAAAAwAKELQAAAACwAGGrkjp48KAeeOAB1ahRQwEBAWrZsqW2bNni2G+MUVxcnCIjI+Xv76/OnTtr165dTufIycnRiBEjVLNmTQUGBqpPnz46cOCAU5/09HTFxsYqJCREISEhio2N1cmTJ5367N+/X71791ZgYKBq1qypkSNHKjc317LHjrI5d+6cnn32WTVs2FD+/v665ppr9MILL8hutzv6MH5QYN26derdu7ciIyNls9n0n//8x2l/RRsrO3bsUKdOneTv76+rr75aL7zwglhHyn0uNX7y8vI0YcIENWvWTIGBgYqMjNTAgQN16NAhp3Mwfiqvy/37c6Hhw4fLZrNp1qxZTu2MH5SZQaVz4sQJU79+fTNo0CCzceNGs2fPHrNq1Srz66+/Ovq8+OKLJigoyHzyySdmx44d5t577zW1a9c2mZmZjj6PPPKIufrqq01CQoLZunWr6dKli2nRooU5d+6co8+dd95poqOjTWJioklMTDTR0dEmJibGsf/cuXMmOjradOnSxWzdutUkJCSYyMhI8/jjj/85TwZcNmXKFFOjRg3zxRdfmD179piPPvrIXHXVVWbWrFmOPowfFPjqq6/MM888Yz755BMjySxfvtxpf0UaKxkZGSY8PNz079/f7Nixw3zyyScmKCjIvPzyy9Y9QbikS42fkydPmjvuuMP861//Mj/99JNJSkoybdu2Na1bt3Y6B+On8rrcvz8Fli9fblq0aGEiIyPNK6+84rSP8YOyImxVQhMmTDC33nprsfvtdruJiIgwL774oqPt7NmzJiQkxLzxxhvGmPP/k/Px8TEffviho8/BgweNl5eXiY+PN8YY89///tdIMhs2bHD0SUpKMpLMTz/9ZIw5/w+hl5eXOXjwoKPP0qVLjZ+fn8nIyCifB4xy1atXL/PQQw85tfXt29c88MADxhjGD4p38ZudijZW5s6da0JCQszZs2cdfaZNm2YiIyON3W4vx2cCpXGpN8sFNm3aZCSZffv2GWMYP/if4sbPgQMHzNVXX2127txp6tev7xS2GD8oD9xGWAl99tlnatOmjf7v//5PYWFhuvHGG/XWW2859u/Zs0epqanq3r27o83Pz0+dOnVSYmKiJGnLli3Ky8tz6hMZGano6GhHn6SkJIWEhKht27aOPu3atVNISIhTn+joaEVGRjr69OjRQzk5OU63NaLiuPXWW/XNN9/o559/liRt375d69evV8+ePSUxflByFW2sJCUlqVOnTk5fUNqjRw8dOnRIe/fuLf8nAOUuIyNDNptN1apVk8T4waXZ7XbFxsbqySef1A033FBoP+MH5YGwVQn9/vvvmjdvnqKiorRixQo98sgjGjlypN577z1JUmpqqiQpPDzc6bjw8HDHvtTUVPn6+qp69eqX7BMWFlbo+mFhYU59Lr5O9erV5evr6+iDimXChAm67777dP3118vHx0c33nijRo8erfvuu08S4wclV9HGSlF9CrYZTxXf2bNn9dRTT2nAgAEKDg6WxPjBpU2fPl1VqlTRyJEji9zP+EF5qOLuAvDns9vtatOmjaZOnSpJuvHGG7Vr1y7NmzdPAwcOdPSz2WxOxxljCrVd7OI+RfUvTR9UHP/617+0ePFiffDBB7rhhhuUnJys0aNHKzIyUg8++KCjH+MHJVWRxkpRtRR3LCqOvLw89e/fX3a7XXPnzr1sf8YPtmzZoldffVVbt251+fVh/MAVzGxVQrVr11bTpk2d2po0aaL9+/dLkiIiIiQV/ktKWlqa468sERERys3NVXp6+iX7HDlypND1jx496tTn4uukp6crLy+v0F94UDE8+eSTeuqpp9S/f381a9ZMsbGxGjNmjKZNmyaJ8YOSq2hjpag+aWlpkgrPvqHiyMvLU79+/bRnzx4lJCQ4ZrUkxg+K99133yktLU316tVTlSpVVKVKFe3bt0/jxo1TgwYNJDF+UD4IW5XQLbfcot27dzu1/fzzz6pfv74kqWHDhoqIiFBCQoJjf25urtauXasOHTpIklq3bi0fHx+nPocPH9bOnTsdfdq3b6+MjAxt2rTJ0Wfjxo3KyMhw6rNz504dPnzY0WflypXy8/NT69aty/mRozycOXNGXl7O/3R4e3s7ln5n/KCkKtpYad++vdatW+e0HPPKlSsVGRnpePOFiqUgaP3yyy9atWqVatSo4bSf8YPixMbG6scff1RycrLjJzIyUk8++aRWrFghifGDcvKnLseBCmHTpk2mSpUq5h//+If55ZdfzJIlS0xAQIBZvHixo8+LL75oQkJCzLJly8yOHTvMfffdV+RyzHXq1DGrVq0yW7duNbfffnuRy6E2b97cJCUlmaSkJNOsWbMil0Pt2rWr2bp1q1m1apWpU6cOS3dXYA8++KC5+uqrHUu/L1u2zNSsWdOMHz/e0YfxgwKnTp0y27ZtM9u2bTOSzMyZM822bdscq8VVpLFy8uRJEx4ebu677z6zY8cOs2zZMhMcHMzSy250qfGTl5dn+vTpY+rUqWOSk5PN4cOHHT85OTmOczB+Kq/L/ftzsYtXIzSG8YOyI2xVUp9//rmJjo42fn5+5vrrrzdvvvmm03673W4mTZpkIiIijJ+fn+nYsaPZsWOHU5/s7Gzz+OOPm9DQUOPv729iYmLM/v37nfocP37c3H///SYoKMgEBQWZ+++/36Snpzv12bdvn+nVq5fx9/c3oaGh5vHHH3da+hQVS2Zmphk1apSpV6+eqVq1qrnmmmvMM8884/TmhvGDAqtXrzaSCv08+OCDxpiKN1Z+/PFHc9tttxk/Pz8TERFh4uLiWHbZjS41fvbs2VPkPklm9erVjnMwfiqvy/37c7GiwhbjB2VlM4avpgYAAACA8sZntgAAAADAAoQtAAAAALAAYQsAAAAALEDYAgAAAAALELYAAAAAwAKELQAAAACwAGELAAAAACxA2AIAAAAACxC2AAAoZwsXLlS1atXcXQYAwM0IWwAAjzVo0CDZbDbZbDb5+PgoPDxc3bp107vvviu73f6n1NCgQQPNmjXLqe3ee+/Vzz///KdcHwBQcRG2AAAe7c4779Thw4e1d+9eff311+rSpYtGjRqlmJgYnTt3rlTnNMaU+lhJ8vf3V1hYWKmPBwBcGQhbAACP5ufnp4iICF199dVq1aqVnn76aX366af6+uuvtXDhQu3du1c2m03JycmOY06ePCmbzaY1a9ZIktasWSObzaYVK1aoTZs28vPz03fffafffvtNf/nLXxQeHq6rrrpKN910k1atWuU4T+fOnbVv3z6NGTPGMcMmFX0b4bx589SoUSP5+vqqcePGev/9953222w2vf322/rb3/6mgIAARUVF6bPPPrPkOQMA/DkIWwCAK87tt9+uFi1aaNmyZS4dN378eE2bNk0pKSlq3ry5Tp8+rZ49e2rVqlXatm2bevTood69e2v//v2SpGXLlqlOnTp64YUXdPjwYR0+fLjI8y5fvlyjRo3SuHHjtHPnTg0fPlyDBw/W6tWrnfpNnjxZ/fr1048//qiePXvq/vvv14kTJ0r3JAAA3I6wBQC4Il1//fXau3evS8e88MIL6tatmxo1aqQaNWqoRYsWGj58uJo1a6aoqChNmTJF11xzjWPGKTQ0VN7e3goKClJERIQiIiKKPO/LL7+sQYMG6dFHH9V1112nsWPHqm/fvnr55Zed+g0aNEj33Xefrr32Wk2dOlVZWVnatGlTqR4/AMD9CFsAgCuSMcZxW19JtWnTxmk7KytL48ePV9OmTVWtWjVdddVV+umnnxwzWyWVkpKiW265xantlltuUUpKilNb8+bNHf8dGBiooKAgpaWluXQtAEDFUcXdBQAAYIWUlBQ1bNhQXl7n/65ojHHsy8vLK/KYwMBAp+0nn3xSK1as0Msvv6xrr71W/v7+uueee5Sbm+tyPRcHv6LCoI+PT6Fj/qxVFQEA5Y+ZLQDAFefbb7/Vjh07dPfdd6tWrVqS5PR5qgsXy7iU7777ToMGDdLf/vY3NWvWTBEREYVuTfT19VV+fv4lz9OkSROtX7/eqS0xMVFNmjQpUR0AAM/EzBYAwKPl5OQoNTVV+fn5OnLkiOLj4zVt2jTFxMRo4MCB8vb2Vrt27fTiiy+qQYMGOnbsmJ599tkSnfvaa6/VsmXL1Lt3b9lsNj333HOFZpoaNGigdevWqX///vLz81PNmjULnefJJ59Uv3791KpVK3Xt2lWff/65li1b5rSyIQDgysPMFgDAo8XHx6t27dpq0KCB7rzzTq1evVqvvfaaPv30U3l7e0uS3n33XeXl5alNmzYaNWqUpkyZUqJzv/LKK6pevbo6dOig3r17q0ePHmrVqpVTnxdeeEF79+5Vo0aNHLNoF/vrX/+qV199VS+99JJuuOEGzZ8/XwsWLFDnzp3L9NgBABWbzVx4EzsAAAAAoFwwswUAAAAAFiBsAQAAAIAFCFsAAAAAYAHCFgAAAABYgLAFAAAAABYgbAEAAACABQhbAAAAAGABwhYAAAAAWICwBQAAAAAWIGwBAAAAgAUIWwAAAABggf8PpdmX22KTIYoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count      5000.000000\n",
      "mean      65519.348200\n",
      "std       15338.105629\n",
      "min       55014.000000\n",
      "25%       61231.000000\n",
      "50%       61304.000000\n",
      "75%       61411.000000\n",
      "max      152841.000000\n",
      "Name: pattern_duration, dtype: float64\n",
      "472  outliers with duration > 65000\n"
     ]
    }
   ],
   "source": [
    "# Pattern duration plot\n",
    "pattern_durations = pattern_df.groupby(['pattern_name']).first()['pattern_duration']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(pattern_durations, bins=5, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Pattern Durations')\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Number of Patterns')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print (pattern_durations.describe())\n",
    "print (pattern_durations[pattern_durations > 65000].shape[0], \" outliers with duration > 65000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54851ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate firing rate distribution over patterns per neuron (mean and SD)\n",
    "seen_patterns = {}\n",
    "firing_rates = {}\n",
    "for neuron_id in spikes_df['neuron_id'].unique():\n",
    "    neuron_spikes = spikes_df[spikes_df['neuron_id'] == neuron_id]\n",
    "    rates = []\n",
    "    for idx, row in pattern_df.iterrows():\n",
    "        if row['pattern_name'] in seen_patterns:\n",
    "            continue # quick and dirty\n",
    "        t_start = row['pattern_flag_start_timestamp']\n",
    "        t_end = row['pattern_end_timestamp']\n",
    "        count = neuron_spikes[(neuron_spikes['timestamp'] >= t_start) & (neuron_spikes['timestamp'] < t_end)].shape[0]\n",
    "        duration_sec = (t_end - t_start) / 30000  # Convert ticks to seconds\n",
    "        rate = count / duration_sec if duration_sec > 0 else 0\n",
    "        rates.append(rate)\n",
    "        seen_patterns[row['pattern_name']] = True\n",
    "    firing_rates[neuron_id] = {\n",
    "        'mean_rate': np.mean(rates),\n",
    "        'std_rate': np.std(rates)\n",
    "    }\n",
    "# firing_rates_df = pd.DataFrame.from_dict(firing_rates, orient='index')\n",
    "# firing_rates_df.columns = ['mean_firing_rate', 'std_firing_rate']\n",
    "\n",
    "# make a violin plot for each neuron of firing rates across patterns\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.violinplot([spikes_df[spikes_df['neuron_id'] == nid]['timestamp'].groupby(\n",
    "#     pd.cut(spikes_df[spikes_df['neuron_id'] == nid]['timestamp'], bins=patterns['pattern_flag_start_timestamp'].tolist() + [last_spike_time])\n",
    "# ).count() / ((patterns['pattern_end_timestamp'] - patterns['pattern_flag_start_timestamp']).values / 30000) for nid in spikes_df['neuron_id'].unique()],\n",
    "#                showmeans=True)\n",
    "# plt.xticks(ticks=np.arange(1, len(spikes_df['neuron_id'].unique()) + 1), labels=spikes_df['neuron_id'].unique(), rotation=90)\n",
    "# plt.xlabel('Neuron ID')\n",
    "# plt.ylabel('Firing Rate (spikes/sec)')\n",
    "# plt.title('Firing Rate Distribution Across Patterns per Neuron')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('patterns_data_firing_rate_distribution_per_neuron.png')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs: 600 ms (10 0.6ms steps) x 44 stimulating channels with 4 delay modes.\n",
    "pattern_stims = {}\n",
    "spike_responses = {}\n",
    "for pattern_name in pattern_df['pattern_name'].unique():\n",
    "    pattern_subset = pattern_df[pattern_df['pattern_name'] == pattern_name]\n",
    "    pattern_start_time = pattern_subset['pattern_flag_start_timestamp'].iloc[0]\n",
    "    pattern_end_time = pattern_subset['pattern_end_timestamp'].iloc[0]\n",
    "    stim = np.zeros((44, 600))  # 44 channels, 600 ms duration\n",
    "    for idx, row in pattern_subset.iterrows(): # process each substep\n",
    "        step_index = row['step_index']\n",
    "        stim_ms = 60 * step_index  # each step is 60 ms\n",
    "        if pd.isna(row['channel']):\n",
    "            continue  # No stimulation for this step\n",
    "        channel_index = channel_to_index[int(row['channel'])]\n",
    "        delay_mode = int(row['delay_mode'])\n",
    "        # TODO: Map delay modes to phase structures and timings\n",
    "        if delay_mode == 0: # 3 pulses, 50 Hz, starting at stim_ms\n",
    "            for pulse in range(3):\n",
    "                pulse_time = stim_ms + pulse * 20  # 20 ms interval for 50 Hz\n",
    "                if pulse_time < 600:\n",
    "                    stim[channel_index, pulse_time] = 3 # 3 uA amplitude\n",
    "        elif delay_mode == 1: # 3 pulses, 50 Hz, starting at 10ms after stim_ms\n",
    "            for pulse in range(3):\n",
    "                pulse_time = stim_ms + 10 + pulse * 20  # 20 ms interval for 50 Hz\n",
    "                if pulse_time < 600:\n",
    "                    stim[channel_index, pulse_time] = 3 # 3 uA amplitude\n",
    "        elif delay_mode == -1: # same as 0 but with reverse phase structure\n",
    "            for pulse in range(3): \n",
    "                pulse_time = stim_ms + pulse * 20  # 20 ms interval for 50 Hz\n",
    "                if pulse_time < 600:\n",
    "                    stim[channel_index, pulse_time] = 3 # 3 uA amplitude\n",
    "        elif delay_mode == 2: # 6 pulses, 100 Hz, starting at stim_ms\n",
    "            for pulse in range(6):\n",
    "                pulse_time = stim_ms + pulse * 10  # 10 ms interval for 100 Hz\n",
    "                if pulse_time < 600:\n",
    "                    stim[channel_index, pulse_time] = 3 # 3 uA amplitude\n",
    "    pattern_stims[pattern_name] = stim\n",
    "    spikes_during_pattern = spikes_df[(spikes_df['timestamp'] >= pattern_start_time) & (spikes_df['timestamp'] < pattern_end_time)]\n",
    "    spike_responses_pattern = np.zeros((len(spiking_neurons), 2000))\n",
    "    for idx, row in spikes_during_pattern.iterrows():\n",
    "        neuron_id = row['neuron_id']\n",
    "        neuron_index = spiking_neuron_to_index[neuron_id]\n",
    "        spike_time = row['timestamp'] - pattern_start_time  # relative to pattern start\n",
    "        if 0 <= spike_time < 2000:\n",
    "            spike_responses_pattern[neuron_index, spike_time] = 1\n",
    "    spike_responses[pattern_name] = spike_responses_pattern\n",
    "    \n",
    "\n",
    "# Example: Plot stimulation pattern for a specific pattern\n",
    "for pattern_name in list(pattern_stims.keys())[:1]:  # just plot first pattern\n",
    "    stim = pattern_stims[pattern_name]\n",
    "    # Plot the stimulation pattern as a heatmap\n",
    "    plt.figure(figsize=(10, 6)) \n",
    "    plt.imshow(stim, aspect='auto', cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar(label='Stimulation Amplitude (uA)')\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Channel Index')\n",
    "    plt.title(f'Stimulation Pattern: {pattern_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e36444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =====================\n",
    "# PyTorch Dataset\n",
    "# =====================\n",
    "class StimSpikeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for stimulation patterns and spike responses.\n",
    "    \n",
    "    Inputs (X): Stimulation patterns of shape (44, 600) - 44 channels x 600 ms\n",
    "    Outputs (Y): Spike responses of shape (47, 2000) - 47 neurons x 2000 time bins\n",
    "    \"\"\"\n",
    "    def __init__(self, pattern_stims, spike_responses, pattern_names=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pattern_stims: dict mapping pattern_name -> (44, 600) numpy array\n",
    "            spike_responses: dict mapping pattern_name -> (47, 2000) numpy array\n",
    "            pattern_names: list of pattern names to include (for train/val/test splits)\n",
    "        \"\"\"\n",
    "        if pattern_names is None:\n",
    "            pattern_names = list(pattern_stims.keys())\n",
    "        \n",
    "        self.pattern_names = pattern_names\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        \n",
    "        for name in pattern_names:\n",
    "            self.X.append(torch.tensor(pattern_stims[name], dtype=torch.float32))\n",
    "            self.Y.append(torch.tensor(spike_responses[name], dtype=torch.float32))\n",
    "        \n",
    "        self.X = torch.stack(self.X)  # (N, 44, 600)\n",
    "        self.Y = torch.stack(self.Y)  # (N, 47, 2000)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pattern_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "# =====================\n",
    "# Create Train/Val/Test Splits\n",
    "# =====================\n",
    "all_pattern_names = list(pattern_stims.keys())\n",
    "print(f\"Total patterns: {len(all_pattern_names)}\")\n",
    "\n",
    "# Split: 70% train, 15% val, 15% test\n",
    "train_names, temp_names = train_test_split(all_pattern_names, test_size=0.3, random_state=42)\n",
    "val_names, test_names = train_test_split(temp_names, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_names)}, Val: {len(val_names)}, Test: {len(test_names)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = StimSpikeDataset(pattern_stims, spike_responses, train_names)\n",
    "val_dataset = StimSpikeDataset(pattern_stims, spike_responses, val_names)\n",
    "test_dataset = StimSpikeDataset(pattern_stims, spike_responses, test_names)\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5e4919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# RNN Model\n",
    "# =====================\n",
    "class StimToSpikeRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN model that maps stimulation patterns to spike responses.\n",
    "    \n",
    "    Input: (batch, 44 channels, 600 time steps) -> transpose to (batch, 600, 44)\n",
    "    Output: (batch, 47 neurons, 2000 time steps)\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 n_stim_channels=44, \n",
    "                 n_neurons=47,\n",
    "                 hidden_size=128,\n",
    "                 num_layers=2,\n",
    "                 dropout=0.2,\n",
    "                 bidirectional=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_stim_channels = n_stim_channels\n",
    "        self.n_neurons = n_neurons\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # RNN processes time steps of stimulation\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=n_stim_channels,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        rnn_output_size = hidden_size * (2 if bidirectional else 1)\n",
    "        \n",
    "        # Project RNN output to spike prediction space\n",
    "        # We need to go from 600 time steps -> 2000 time steps\n",
    "        self.temporal_upsample = nn.Sequential(\n",
    "            nn.Linear(rnn_output_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        # Upsample temporally: 600 -> 2000\n",
    "        self.time_project = nn.Linear(600, 2000)\n",
    "        \n",
    "        # Final projection to neurons\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(256, n_neurons),\n",
    "            nn.Sigmoid()  # Spike probabilities\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, 44, 600) - stimulation patterns\n",
    "        Returns:\n",
    "            y: (batch, 47, 2000) - predicted spike responses\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Transpose to (batch, time, channels) for RNN\n",
    "        x = x.permute(0, 2, 1)  # (batch, 600, 44)\n",
    "        \n",
    "        # Pass through RNN\n",
    "        rnn_out, _ = self.rnn(x)  # (batch, 600, hidden_size)\n",
    "        \n",
    "        # Project hidden states\n",
    "        projected = self.temporal_upsample(rnn_out)  # (batch, 600, 256)\n",
    "        \n",
    "        # Transpose and upsample time: 600 -> 2000\n",
    "        projected = projected.permute(0, 2, 1)  # (batch, 256, 600)\n",
    "        projected = self.time_project(projected)  # (batch, 256, 2000)\n",
    "        projected = projected.permute(0, 2, 1)  # (batch, 2000, 256)\n",
    "        \n",
    "        # Project to neurons\n",
    "        output = self.output_layer(projected)  # (batch, 2000, 47)\n",
    "        \n",
    "        # Transpose to match target shape\n",
    "        output = output.permute(0, 2, 1)  # (batch, 47, 2000)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = StimToSpikeRNN(\n",
    "    n_stim_channels=44,\n",
    "    n_neurons=47,\n",
    "    hidden_size=128,\n",
    "    num_layers=1\n",
    "    dropout=0.2,\n",
    "    bidirectional=False\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "sample_x, sample_y = next(iter(train_loader))\n",
    "sample_x = sample_x.to(device)\n",
    "with torch.no_grad():\n",
    "    sample_out = model(sample_x)\n",
    "print(f\"Input shape: {sample_x.shape}\")\n",
    "print(f\"Output shape: {sample_out.shape}\")\n",
    "print(f\"Target shape: {sample_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47306813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Training Loop\n",
    "# =====================\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_x)\n",
    "        loss = criterion(predictions, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "    \n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            predictions = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            total_loss += loss.item() * batch_x.size(0)\n",
    "    \n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "# Training configuration\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# Loss function - BCE since spike responses are binary\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "PATIENCE = 10\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1:3d}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.6f} | \"\n",
    "          f\"Val Loss: {val_loss:.6f} | \"\n",
    "          f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'best_stim_spike_model.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training complete! Best validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Plot Training History\n",
    "# =====================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', color='blue')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', color='orange')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (BCE)')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Learning rate\n",
    "axes[1].plot(history['lr'], color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d5c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Evaluate on Test Set\n",
    "# =====================\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_stim_spike_model.pt'))\n",
    "\n",
    "test_loss = validate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss (BCE): {test_loss:.6f}\")\n",
    "\n",
    "# Generate predictions on test set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        preds = model(batch_x)\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_targets.append(batch_y)\n",
    "\n",
    "all_preds = torch.cat(all_preds, dim=0)\n",
    "all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "print(f\"Test predictions shape: {all_preds.shape}\")\n",
    "print(f\"Test targets shape: {all_targets.shape}\")\n",
    "\n",
    "# Visualize a sample prediction vs ground truth\n",
    "sample_idx = 0\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Ground truth\n",
    "im0 = axes[0].imshow(all_targets[sample_idx].numpy(), aspect='auto', cmap='hot', interpolation='nearest')\n",
    "axes[0].set_title(f'Ground Truth Spike Response (Test Sample {sample_idx})')\n",
    "axes[0].set_xlabel('Time (ms)')\n",
    "axes[0].set_ylabel('Neuron Index')\n",
    "plt.colorbar(im0, ax=axes[0], label='Spike')\n",
    "\n",
    "# Prediction\n",
    "im1 = axes[1].imshow(all_preds[sample_idx].numpy(), aspect='auto', cmap='hot', interpolation='nearest')\n",
    "axes[1].set_title(f'Predicted Spike Response (Test Sample {sample_idx})')\n",
    "axes[1].set_xlabel('Time (ms)')\n",
    "axes[1].set_ylabel('Neuron Index')\n",
    "plt.colorbar(im1, ax=axes[1], label='Spike Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_prediction_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
